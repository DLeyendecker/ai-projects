# -*- coding: utf-8 -*-
"""IA Generativa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F2mey37subNb8YtKL8sl_iyM5n_XcXEH

## Constru√ß√£o de Pipeline RAG para IA Generativa com Databricks e Python

## 1. Configura√ß√£o de Ambiente

Nesta etapa, realizamos a instala√ß√£o e importa√ß√£o dos pacotes necess√°rios para execu√ß√£o do pipeline.
O ambiente foi preparado de forma a garantir todas as depend√™ncias compat√≠veis para o processamento de dados e a opera√ß√£o dos modelos de IA.
"""

!pip install -q "numpy==1.25.2" databricks-langchain langchain_milvus langchain-huggingface sentence-transformers beautifulsoup4

!pip install -q watermark

!pip install -q databricks-cli databricks-sql-connector

import os
os.environ["USER_AGENT"] = "LangChain (Google Colab)"

# Imports
import bs4
import sentence_transformers
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from databricks_langchain import ChatDatabricks
from langchain_milvus import Milvus
import warnings
warnings.filterwarnings('ignore')

print("Ambiente Colab funcionando com sucesso.")

"""## 2. Configura√ß√£o de Credenciais de Acesso

Nesta etapa, s√£o definidas as credenciais de autentica√ß√£o necess√°rias para conex√£o segura com o ambiente Databricks, garantindo acesso autorizado aos recursos da plataforma.

"""

# Defina as credenciais do Databricks
os.environ["DATABRICKS_HOST"] = "https://dbc-baba4cbd-2594.cloud.databricks.com/"   # Substitua pela URL do seu Databricks
os.environ["DATABRICKS_TOKEN"] = "DATABRICKS_TOKEN"           # Substitua pelo token gerado

"""## 3. Extra√ß√£o de Dados da Web e Cria√ß√£o de Chunks

Realiza-se a extra√ß√£o dos dados a partir da fonte web definida. Em seguida, o conte√∫do bruto √© segmentado em pequenos blocos (chunks) de texto, otimizando o processamento e a gera√ß√£o de embeddings para consultas posteriores.

"""

# Cria o carregador de dados da web
loader = WebBaseLoader("https://mitsloanreview.com.br/tecnologia/")

# Executa o carregador e extrai os dados da web
documentos = loader.load()

# Cria o separador de texto
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1800, chunk_overlap = 200)

# Aplica o separador e cria os chunks (documentos)
docs = text_splitter.split_documents(documentos)

len(docs)

"""## 4. Carregamento do Modelo de Embeddings

Inicializa-se o modelo de embeddings pr√©-treinado, que transforma os blocos de texto em vetores num√©ricos. Essa etapa √© fundamental para permitir buscas sem√¢nticas no banco vetorial.

https://huggingface.co/BAAI/bge-small-en-v1.5
"""

# Carrega o modelo de embeddings
embeddings = HuggingFaceEmbeddings(model_name = "BAAI/bge-small-en-v1.5")

"""## 5. Cria√ß√£o e Carregamento do Banco de Dados Vetorial

Configura-se o banco de dados vetorial, respons√°vel por armazenar e indexar os embeddings gerados. Essa estrutura viabiliza a recupera√ß√£o eficiente dos dados relevantes para a gera√ß√£o de respostas.
"""

# Cria o banco de dados vetorial
vector_db = Milvus.from_documents(documents = docs,
                                      embedding = embeddings,
                                      collection_name = 'collection',
                                      index_params = {"index_type": "FLAT"},
                                      connection_args = {"uri": "./milvus.db"})

# Cria o retriever para recuperar os dados do Vector DB
retriever = vector_db.as_retriever()

retriever

# Testando o Retriever

# Define uma frase
query = "Quais s√£o as tend√™ncias de tecnologia discutidas no blog?"

# Busca texto similar a frase dentro do banco vetorial
vector_db.similarity_search(query, k = 1)

"""## 6. Definindo o Endpoint do Modelo de Linguagem (LLM)

Nesta etapa, configuramos o endpoint do LLM hospedado no Databricks, que ser√° utilizado para a gera√ß√£o das respostas a partir dos dados recuperados do banco vetorial.
"""

# Definimos aqui o endpoint para o LLM no Databricks
llm = ChatDatabricks(endpoint = "databricks-dbrx-instruct", max_tokens = 200)

"""## 7. Definindo o Prompt Template

Configuramos o template de prompt que orienta o modelo de linguagem na constru√ß√£o das respostas, garantindo consist√™ncia e direcionamento conforme o contexto dos dados extra√≠dos.
"""

# Cria o texto do prompt
PROMPT_TEMPLATE = """
        Humano: Voc√™ √© um assistente de IA e fornece respostas a perguntas do usu√°rio.

        Use as seguintes informa√ß√µes para fornecer uma resposta concisa √† pergunta entre as tags <question>.
        Se voc√™ n√£o sabe a resposta, apenas diga que n√£o sabe, n√£o tente inventar uma resposta.

        <context>
        {context}
        </context>

        <question>
        {question}
        </question>

        A resposta deve ser espec√≠fica e usar apenas informa√ß√µes confi√°veis.

        Assistente:"""

# Cria o prompt template
prompt = PromptTemplate(template = PROMPT_TEMPLATE, input_variables = ["context", "question"])

# Cria o retriever
retriever = vector_db.as_retriever()

# Fun√ß√£o para formatar os dados
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

"""## 8. Definindo o RAG Chain para o Pipeline

Montamos a cadeia (chain) de componentes respons√°vel por integrar a recupera√ß√£o dos dados relevantes (Retrieval) e a gera√ß√£o de respostas (Generation) no pipeline RAG.
"""

# RAG Chain

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

"""O `RunnablePassthrough` no LangChain √© um componente que simplesmente encaminha os dados de entrada para a sa√≠da sem realizar altera√ß√µes. Funciona como um "canal direto" de passagem. Neste projeto, utilizamos o `RunnablePassthrough` para transmitir a quest√£o do usu√°rio sem modifica√ß√µes at√© a pr√≥xima etapa do pipeline.

## 9. Executando o Pipeline

Com todos os componentes configurados, a execu√ß√£o do pipeline consiste em enviar uma pergunta, process√°-la por meio da arquitetura RAG e retornar a resposta gerada com base nos documentos carregados.
"""

# Invoca a chain
resposta = rag_chain.invoke(query)

resposta

# Caixa de digita√ß√£o para fazer perguntas
print("\nüîé Pergunte algo sobre o conte√∫do carregado:")
question = input("> Sua pergunta: ")

print("\n‚è≥ Gerando resposta...")
resposta = rag_chain.invoke(question)

print("\nüì¢ Resposta:\n")
for linha in resposta.split('\n'):
    print(linha)

"""# Fim"""