
# Projeto: Constru√ß√£o de um Pipeline RAG com LLM Databricks

## Vis√£o Geral
Neste projeto, desenvolvi um **pipeline completo** de **Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)** utilizando **Databricks**, **Python**, **Google Colab** e **bancos vetoriais**.

O objetivo foi integrar **IA Generativa** com mecanismos de **recupera√ß√£o de dados**, aumentando a **precis√£o**, a **atualiza√ß√£o** e a **confiabilidade** das respostas geradas por modelos de linguagem (LLMs).

---

## Objetivos
- Implementar um **pipeline RAG** de ponta a ponta.
- Utilizar **Databricks LLM** para gera√ß√£o de respostas.
- Usar **Web Scraping** para ingest√£o de dados externos.
- Criar um **banco vetorial** para buscas sem√¢nticas.
- Orquestrar a consulta, recupera√ß√£o e gera√ß√£o de informa√ß√µes atualizadas.

---

## Estrutura do Projeto

### 1. Configura√ß√£o de Ambiente
Instala√ß√£o dos pacotes essenciais:  
- `databricks-langchain`, `langchain_milvus`, `langchain-huggingface`, `sentence-transformers`, `beautifulsoup4`, `databricks-cli`, entre outros.

O ambiente foi preparado no **Google Colab** para r√°pida execu√ß√£o e compatibilidade.

### 2. Configura√ß√£o de Credenciais
- Vari√°veis de ambiente para conectar ao **Databricks** via token seguro.

### 3. Extra√ß√£o de Dados
- Utiliza√ß√£o do `WebBaseLoader` para capturar conte√∫do atualizado de fontes web (ex.: **MIT Sloan Review Brasil**).
- Aplica√ß√£o de **Text Splitting** para segmentar o conte√∫do em pequenos chunks.

### 4. Embeddings
- Carregamento do modelo de embeddings **BAAI/bge-small-en-v1.5** do **HuggingFace**.
- Convers√£o de textos em vetores num√©ricos para posterior recupera√ß√£o sem√¢ntica.

### 5. Banco de Dados Vetorial
- Implementa√ß√£o do **Milvus** como banco vetorial local.
- Indexa√ß√£o e armazenamento dos embeddings para buscas r√°pidas.

### 6. Conex√£o com LLM do Databricks
- Configura√ß√£o do endpoint **databricks-dbrx-instruct**.
- Limita√ß√£o controlada de `max_tokens` para respostas otimizadas.

### 7. Constru√ß√£o do Prompt Template
- Defini√ß√£o de um **prompt seguro e direcionado**.
- Garantia de que a IA s√≥ responder√° com informa√ß√µes baseadas no contexto recuperado.

### 8. Constru√ß√£o do RAG Chain
- Integra√ß√£o entre **retriever**, **prompt** e **LLM** usando LangChain.
- Fluxo de dados totalmente encadeado, passando pela recupera√ß√£o, formata√ß√£o, prompting e gera√ß√£o.

### 9. Execu√ß√£o Final
- Interface simples via terminal para perguntas em tempo real.
- Pipeline executando a consulta RAG de ponta a ponta.

---

## Desafios Enfrentados
- **Gerenciamento de depend√™ncias** para garantir compatibilidade no ambiente Colab.
- **Tokeniza√ß√£o correta** das entradas para evitar erros no modelo LLM.
- **Ajuste do tamanho dos chunks** para maximizar o aproveitamento sem estourar o limite de tokens.
- **Cria√ß√£o manual de √≠ndices vetoriais** para otimizar a busca no Milvus local.
- **Tratamento de exce√ß√µes** na comunica√ß√£o entre o pipeline e o endpoint Databricks.

---

## Tecnologias Utilizadas
- **Python** (LangChain, HuggingFace, Milvus)
- **Databricks** (ChatDatabricks Endpoint)
- **Google Colab**
- **Milvus** (Banco Vetorial)
- **BeautifulSoup4** (Web Scraping)
- **LangChain Framework** (Orquestra√ß√£o)

---

## Benef√≠cios do Pipeline
- **Atualiza√ß√£o Cont√≠nua**: Capta dados web em tempo real sem necessidade de retreinar o modelo.
- **Confiabilidade**: Baseia as respostas em fontes verific√°veis externas.
- **Personaliza√ß√£o**: Pode ser adaptado para diversos dom√≠nios (jur√≠dico, sa√∫de, financeiro, etc).

---

## Pr√≥ximos Passos
- Automatizar ingest√£o cont√≠nua de fontes web.
- Substituir Milvus local por **Milvus Cloud** ou **Pinecone** para escalar a solu√ß√£o.
- Implementar cache inteligente para reduzir lat√™ncia de resposta.
- Testar varia√ß√µes de prompt engineering para diferentes n√≠veis de formalidade nas respostas.

---

## Contato
Davi Leyendecker  
üìß d.leyendecker@ymail.com
